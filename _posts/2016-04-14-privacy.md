---
title: Data Privacy and ML
---

In data privacy debates, I sometimes hear a distinction between "personal" data and other types of information tied to a user account. The feeling, as I understand it, is that certain sensitive types of personal information -- attributes like race, annual income, gender, sexual preference, religious beliefs, etc. -- should be a special, protected class of data, deserving of particular care and stewardship. By exclusion, then, other forms of data would have less stringent storage and sharing rules. As a hypothetical example: It would clearly be bad if every employee at TurboTax had full access to the annual income and social security number of each tax filer, but maybe we're less worried about employees having access to the date at which tax returns were filed.

I don't think it's bad to be especially vigilent of "personal" data, but I do think it's overly simplistic to think that there's a clear dividing line between what's personal and what's not. The reality is that "innocuous" data is often enough to predict extremely "personal" data.

[Redlining](https://en.wikipedia.org/wiki/Redlining) is a prominent historical example. It's illegal to discriminate based on race, but if you're allowed to deny a bank loan based on home address, and if geographic location correlates heavily with race... The data point here is that geography predicts race; since we want to protect race as personal data, we then also need to protect geographic data.

A more recent example is the idea of [using NYC's open taxi cab data to predict which cab drivers are Muslim](https://www.reddit.com/r/dataisbeautiful/comments/2t201h/identifying_muslim_cabbies_from_trip_data_and/). There isn't a source of truth here, so we don't know how accurate this prediction is -- but I find it convincing as a proof of concept. We want to predict religious beliefs as personal data, so perhaps we also need to protect the pick-up/drop-off data timestamps for cab drivers.

The point is that data which doesn seem personal or sensitive at first could, in fact, be used to accurately predict data that is **very** personal or sensitive. If you want to effectively protect the variable `y`, then you may also need to protect any set of variables `x1`, `x2`, ..., `xn` that can be used to **predict** `y`. This is a hard problem, because there are lots and lots of things that might predict `y`.
